import argparse
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PATH = os.path.join(BASE_DIR, "chroma")

CHAT_TEMPLATE = """You are a friendly concierge at Grand Vista Hotel. Answer the guest's question naturally and professionally, 
as if you work at the hotel and have this knowledge. Do NOT mention "context", "according to the information", or "based on".
Just answer directly and conversationally like a real hotel staff member would.

Hotel Information:
{context}

Guest's Question: {question}

Your response (speak naturally as hotel staff):"""


def main():
    print("\n" + "="*60)
    print("    GRAND VISTA HOTEL - CHATBOT ASSISTANT")
    print("="*60)
    print("\nWelcome! I'm your hotel concierge assistant.")
    print("Ask me anything about Grand Vista Hotel.")
    print("Type 'quit' or 'exit' to end the conversation.\n")
    
    # Prepare the DB
    embedding_function = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
    
    # Initialize the model
    model = Ollama(model="llama2")
    
    # Chat loop
    while True:
        try:
            user_input = input("\nüë§ You: ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:
                print("\nüè® Assistant: Thank you for choosing Grand Vista Hotel! Have a great day!")
                break
            
            # Search the DB (using regular similarity search to avoid score warnings)
            results = db.similarity_search(user_input, k=3)
            
            if len(results) == 0:
                print("\nüè® Assistant: I couldn't find relevant information about that. Please contact our front desk at +1 (212) 555-0100 for more help.")
                continue
            
            # Get context from results
            context_text = "\n\n---\n\n".join([doc.page_content for doc in results])
            
            # Create prompt
            prompt_template = ChatPromptTemplate.from_template(CHAT_TEMPLATE)
            prompt = prompt_template.format(context=context_text, question=user_input)
            
            # Get response from LLM
            print("\nüè® Assistant: ", end="", flush=True)
            response_text = model.invoke(prompt)
            print(response_text)
            
        except KeyboardInterrupt:
            print("\n\nüè® Assistant: Thank you for using our chat service! Goodbye!")
            break
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error: {str(e)}")
            print("Please try again or contact the front desk.")


if __name__ == "__main__":
    main()
